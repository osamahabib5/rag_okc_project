# rag_okc_project  
**End-to-End Retrieval-Augmented Generation (RAG) Pipeline for NBA Stats**

---

## ðŸŽ¯ Project Overview  
This project builds a full pipeline that lets you ask natural language questions about NBA game data and get answers grounded in that data.  

It consists of:  
- A backend for ingestion, embedding, retrieval, and answer generation  
- A frontend chat interface for user interaction  
- Deployment setup with Docker and Docker Compose  

**Use Case:**  
You have CSVs of NBA game data (2023-24 & 2024-25 seasons, Western Conference teams) â†’ store in PostgreSQL â†’ create embeddings â†’ retrieve relevant rows with semantic search (`pgvector`) â†’ feed context + user question into an LLM â†’ get an answer with evidence from the retrieved data.  

---

## ðŸ§± Architecture & Key Components  

### 1. Data Ingestion  
- **File:** `backend/ingest.py`  
- Loads CSV files into PostgreSQL tables (game summaries, box scores, etc.)  
- Uses Docker-hosted PostgreSQL defined in `docker-compose.yml`  

### 2. Embeddings  
- **File:** `backend/embed.py`  
- Reads rows from the database  
- Generates embeddings using `nomic-embed-text` via Ollama  
- Stores embeddings in PostgreSQL using the `pgvector` extension  

### 3. Retrieval & Answer Generation  
- **File:** `backend/rag.py`  
- Given a question:  
  - Uses semantic retrieval to find relevant context rows  
  - Builds a prompt combining the user question and retrieved data  
  - Calls an LLM (e.g., `llama3.2:3b`) via Ollama  
  - Outputs an answer with an `evidence` list showing which rows were used  

### 4. Frontend Chat Interface  
- **Folder:** `frontend/`  
- Built with Angular  
- Provides a simple chat UI that communicates with the backend API  
- Default local server: [http://localhost:4200](http://localhost:4200)  

### 5. Deployment via Docker  
- **Files:**  
  - `docker-compose.yml` â€“ defines database, model, and app services  
  - `Dockerfile` â€“ builds the app image  

---

## ðŸš€ Quick Start  

### Prerequisites  
- Docker Desktop installed and running  
- Node.js 16.x or higher (for the frontend)  

### Backend Setup  
```bash
git clone https://github.com/osamahabib5/rag_okc_project.git
cd rag_okc_project

# Start database and model containers
docker compose up -d db ollama

# Pull required models into Ollama
docker exec ollama ollama pull nomic-embed-text
docker exec ollama ollama pull llama3.2:3b

# Build the app container
docker compose build app
```

### Running the Pipeline

#### Ingest data
```bash
docker compose run --rm app python -m backend.ingest
```

#### Embed data
```bash
docker compose run --rm app python -m backend.embed
```
***Note: This step may take significant time depending on hardware.***

#### Run RAG script (answer the pre-set questions)
```bash
docker compose run --rm app python -m backend.rag
```
#### Launching Frontend & API Server
```bash
# Start backend API server
docker compose run --rm --service-ports app uvicorn backend.server:app --host 0.0.0.0 --port 8000 --reload

# Then, in a new terminal:
cd frontend
npm install --force
npm start

# Visit http://localhost:4200 in your browser
```

### Project Structure
```bash
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ ingest.py       # CSV â†’ PostgreSQL
â”‚   â”œâ”€â”€ embed.py        # Generate & store embeddings
â”‚   â”œâ”€â”€ rag.py          # Retrieval + LLM answering script
â”‚   â””â”€â”€ server.py       # FastAPI/UVicorn backend API (for chat interface)
â”œâ”€â”€ frontend/           # Angular chat interface
â”œâ”€â”€ part1/              # Part 1 assignment assets (questions.json etc)
â”œâ”€â”€ part2/              # Part 2 assets (UI demo video etc)
â”œâ”€â”€ part3/              # Part 3 â€“ write-up responses
â”œâ”€â”€ part4/              # Optional fine-tuning of embeddings
â”œâ”€â”€ prompts/            # Directory to list any AI prompts used
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
## ðŸŽ¯ Final Output

- Part 1 (Backend RAG): Run the pipeline and answer the 10 game-level prompts in part1/questions.json. Save results in answers.json, each with evidence.
- Part 2 (Frontend): Provide a chat interface that interacts with the backend retrieval+LLM pipeline.

## ðŸ§  Why This Matters
This project combines structured sports data, semantic embeddings, vector-search retrieval, and LLMs to create a user-facing application. It demonstrates skills in:

- Data ingestion and relational database design
- Embedding generation and vector search (via pgvector)
- Prompt engineering and LLM grounding in factual data
- Full stack development (backend API + frontend chat UI)
- DevOps / containerisation with Docker
- (Optional) Embedding fine-tuning and retrieval evaluation

## ðŸ”§ Customisation & Next Steps

- Change or extend the dataset (e.g., include Eastern Conference, other seasons)
- Use a different embedding model or vector store
- Replace Llama3.2 with a larger/smaller LLM depending on resources
- Improve frontend UI/UX: add chat history, user authentication, interactive visualisations
- Deploy to the cloud (AWS/GCP/Azure) and expose via a web app
- Add metrics logging: embedding latency, retrieval recall, user feedback loop

